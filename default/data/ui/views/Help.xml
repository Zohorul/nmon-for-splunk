<form stylesheet="Help.css,Home.css,hover.css,singlevalue.css,hide_timeindicator.css">
  <label>Help</label>
  <row>
    <html>

		<br />

		<div class="round-button">
			<a href="Home">
			<img src="../../static/app/nmon/icons/Home_Icon.svg" alt="Home"/>
			</a>
		</div>

      <div style="text-align: center;">
        <img src="../../static/app/nmon/logos/NMON_simplelogo.svg" alt="logo"/>
      </div>
      
      <div class="logosubtitle">        
        <h2>Performance Monitor for Unix and Linux Systems</h2>
		</div>

		<br />
		<br />

    </html>
  </row>
  <row>
    <html>


<p>Copyright 2014 Guilhem Marchand	<br />
	<br />
   Licensed under the Apache License, Version 2.0 (the "License");<br />
   you may not use this file except in compliance with the License.<br />
   You may obtain a copy of the License at<br />
	<br />
     http://www.apache.org/licenses/LICENSE-2.0<br />
	<br />
   Unless required by applicable law or agreed to in writing, software<br />
   distributed under the License is distributed on an "AS IS" BASIS,<br />
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br />
   See the License for the specific language governing permissions and<br />
   limitations under the License.<br />
	<br />
	
<b>nmon release 1.5.12</b>

<br />
<br />

<a href="../../static/app/nmon/releases-note.txt" target="_blank">See Releases Notes</a>

</p>

<p>

<br />

<br />
<img src="../../static/app/nmon/icons/grey_theme/bug.png" alt="bug"/>
<br />

<br />
<b>Bug to report ? Feature request ? Contact me: </b> <a href="mailto:guilhem.marchand@gmail.com ?subject=[NMON Splunk App]">guilhem.marchand@gmail.com</a>

<br />

</p>

<br />

<p>

<br />
<img src="../../static/app/nmon/icons/grey_theme/people.png" alt="people"/>
<br />

<br />
<b>Looking for Community help ? See:</b>

<br />

<lu>

<li>Questions on Splunk Base: <a href="http://answers.splunk.com/answers/app/1753" target="_blank">http://answers.splunk.com/answers/app/1753</a></li>

<li>Join the Google Group: <a href="https://groups.google.com/d/forum/nmon-splunk-app" target="_blank">https://groups.google.com/d/forum/nmon-splunk-app</a></li>

</lu>

<br />

</p>



	</html>
  </row>
  <row>
    <html>
	 
<h1>Welcome in Splunk for NMON, Performance Monitor for Unix and Linux Systems</h1>

<br />

<h2>TABLE OF CONTENT</h2>

	<br />

	<ol>

		<li><a href="#INTRODUCTION">INTRODUCTION</a></li>

		<br />

		<li><a href="#PREREQUISITES">PREREQUISITES</a></li>

		<br />

		<li><a href="#INSTALLATION">INSTALLATION</a>

		<br />
		<br />		

		<ol>

			<li><a href="#STANDARD_INSTALLATION">STANDARD INSTALLATION</a></li>

			<br />

			<li><a href="#WINDOWS_SPECIFIC_INSTRUCTIONS">WINDOWS SPECIFIC INSTRUCTIONS</a></li>

		</ol>

		</li>

		<br />

		<li><a href="#DEPLOYMENT_SCENARIOS">DEPLOYMENT SCENARIOS</a>

		<ol>

			<br />

			<li><a href="#DEPLOYMENT_SCENARIOS_DISTRIBUTED">SIMPLE DISTRIBUTED ENVIRONMENT</a></li>

			<br />

			<li><a href="#DEPLOYMENT_SCENARIOS_CLUSTER">CLUSTER CONFIGURATION</a></li>

			<br />

			<li><a href="#DEPLOYMENT_SCENARIOS_CENTRAL">USING CENTRAL SHARES</a></li>

			<br />

			<li><a href="#DEPLOYMENT_SCENARIOS_CENTRAL_CLUSTER">USING CENTRAL SHARES IN CLUSTER ENVIRONMENT WITH FORWARDERS</a></li>

			<br />

		</ol>

		</li>

		<li><a href="#ADVANCED_CONFIGURATION">ADVANCED CONFIGURATION</a>
		
		<ol>

			<br />

			<li><a href="#ADVANCED_ACCURACY">CONFIGURE NMON (use of nmon.conf to customize interval, snapshot and more)</a></li>

			<br />

			<li><a href="#ACTIVATE_NFS">ACTIVATE NFS STATISTICS GENERATION</a></li>

			<br />

			<li><a href="#ADVANCED_FRAMEID">FRAMEID: MAP HOSTS WITH THEIR FRAME IDENTIFIER (example: Pseries with hostnames)</a></li>

			<br />

			<li><a href="#ADVANCED_SPAN">CUSTOMIZE SPAN FEATURE</a></li>

			<br />

		</ol>
		
		</li>

		<li><a href="#USAGE">USAGE</a>
		
		<ol>

			<br />

			<li><a href="#USAGE_COLLECT">NMON COLLECT</a></li>

			<br />

			<li><a href="#USAGE_PROCESSING">NMON PROCESSING</a></li>

			<br />

			<li><a href="#USAGE_DATA">NMON DATA</a></li>

			<br />

		</ol>
		
		</li>

		<li><a href="#UPGRADE_INSTRUCTIONS">UPGRADE INSTRUCTIONS</a></li>

		<br />

		<li><a href="#REFERENCE_MATERIAL">REFERENCE MATERIAL</a></li>

		<br />

		<li><a href="#FAQ">FAQ</a>
		
		<ol>

			<br />
				
			<li><a href="#FAQ_nmon2csv_debug">Debugging nmon2csv Python and Perl converters</a></li>

			<br />
				
			<li><a href="#FAQ_customize">Nmon_SplunkApp_Customize.py: Customize the Application</a></li>

			<br />
				
			<li><a href="#FAQ_customize_extraction">Performance Monitors extraction: Customize Monitors to extract (reduce Data volume and licence costs)</a></li>

			<br />
				
			<li><a href="#FAQ_converter">Python vs Perl converter: Change default converter to Perl alternative</a></li>
		
		</ol>		
		
		
		</li>

	</ol>

	<br />
	 
	</html>
  </row>
  <row>
    <html>	

<div id="INTRODUCTION">

<h1>1. INTRODUCTION</h1>

</div>

<br />

<p>
NMON is short for Nigel's Performance Monitor and is available on AIX Systems, Solaris (with Sarmon), Linux and now ARM Systems.
<br />
This is a great all in one Performance Monitor tool which provides a very large amount of system performance informations and can be used in different scenarios.
<br />
<br />
The classical way to use NMON, running the "nmon" command in terminal, opens a Real time monitoring interface, giving you access to many system informations within a single screen:
<br />

<br />
<img src="../../static/app/nmon/various/nmon_screen.png" alt="Nmon"/>
<br />

<br />
<img src="../../static/app/nmon/various/nmon_screen2.png" alt="Nmon2"/>
<br />


<br />
Beyond this terminal interface, NMON is very often used as a Capacity Planning and Performance tool by running NMON in csv generating mode all along it's run time, for later cold Analyse.
</p>


<p>
There is very few (or none) solutions to Analyse these data with a global and historical vision (Excel has its limits), fortunately Splunk's power is here and this Application
will, i hope, answer to your needs.
</p>

<p>

<h2>
Here are some useful links about NMON:
</h2>

<a href="http://nmon.sourceforge.net/pmwiki.php" target="_blank">http://nmon.sourceforge.net/pmwiki.php</a>

<br />
<br />

<a href="http://www.ibm.com/developerworks/aix/library/au-analyze_aix" target="_blank">http://www.ibm.com/developerworks/aix/library/au-analyze_aix</a>

<br />

<br />

</p>

<p>

Analysing NMON csv data is not easy because of a very specific format Splunk cannot directly manage. (One big problem stands in the event timestamp identification which is very uncommon and defined by a non timestamp pattern)<br />

This is why i decided to develop this App, based on my own professional experience in Unix systems Capacity Planning, to provide to anyone interested a powerful too to Analyse NMON data with an Enterprise Class Application.<br />

<h2>In a few words, here is how the App works:</h2>

<lu>

<li>
After installation, the App is ready to be used, out of the Box
</li>

<li>
Default installation has a file monitor that watches for any new nmon file located in "/opt/splunk/etc/apps/nmon/var/nmon_repository"
</li>

<li>
When a new file is found by Splunk Archive Processor (such as any monitored file or directory), Splunk will call a third party Python (or alternative Perl) script
</li>

<li>
Converter scripts "nmon2csv.py / nmon2csv.pl" will translate nmon data into several csv files in "/opt/splunk/etc/apps/nmon/var/csv_repository"
</li>

<li>
By default, Splunk will watch for this this directory running in "batch" mode, meaning any csv file within this directory will be indexed then deleted (you should not need to keep these files)
</li>

<li>
Once indexed, NMON Data will be ready to be analysed within any available views
</li>

</lu>

</p>

<br />

<p>

<h2>You can verify NMON workflow collecting and indexing by requesting on index with nmon collect/processing sourcetype:</h2>

<pre>
    	index="nmon" sourcetype="nmon_collect"
</pre>

<h2>And:</h2>

<pre>
    	index="nmon" sourcetype="nmon_processing"
</pre>

<br />
<br />
<b>
The "nmon_collect" sourcetype contains iteration of nmon command launched by Splunk, if you collect NMON performance data in your host.
</b>
<br />

<br />
<b>
The "nmon_processing" sourcetype contains outputs of the converter script.
This contains various information about nmon conversion steps (automatically extracted by Splunk when searching for the sourcetype) such as:
</b>

<br />
<br />

<lu>

<li>nmon2csv_version: Version of the Python converter</li>
<li>hostname: hostname of the Nmon host being extracted</li>
<li>nbr_lines: Number of lines in the Nmon file</li>
<li>size_in_bytes: Size of the Nmon file</li>
<li>elapsed_in_seconds: Data processing time in seconds</li>
<li>Nmon_version: Version of Nmon binary</li>
<li>Time_of_Nmon_Data: Time of Nmon Data (extracted from the Nmon file)</li>
<li>Date_of_Nmon_Data: Date of Nmon Data (extracted from the Nmon file)</li>
<li>INTERVAL: Interval between 2 Nmon measures</li>
<li>SNAPSHOTS: Total number of Nmon measures</li>
<li>logical_cpus: Logical number of CPUs (extracted from the Nmon file)</li>
<li>virtual_cpus: Virtual number of CPUs (extracted from the Nmon file)</li>
<li>Nmon_ID: ID of the Nmon file composed by DATE:TIME:hostname:SN</li>

</lu>


<h2>Accessing Performance Metrics Raw data will be achieved as follows:</h2>

<pre>
		index="nmon" sourcetype="nmon_data"
</pre>

<br />
<br />

<b>
Performance Data is identified by it's "type" field and indexed in "nmon" Splunk index, currently here are NMON sections (type field) threaten by the third party script:
</b>


<br />
<br />

<lu>

<li>CPU_ALL</li>
<li>DISKBSIZE (Up to 10 sections, 10 x 150 devices)</li>
<li>DISKBUSY (Up to 10 sections, 10 x 150 devices)</li>
<li>DISKREAD (Up to 10 sections, 10 x 150 devices)</li>
<li>DISKWRITE (Up to 10 sections, 10 x 150 devices)</li>
<li>DISKXFER (Up to 10 sections, 10 x 150 devices)</li>
<li>DISKRIO (Up to 10 sections, 10 x 150 devices)</li>
<li>DISKWRIO (Up to 10 sections, 10 x 150 devices)</li>
<li>FILE</li>
<li>IOADAPT</li>
<li>JFSFILE</li>
<li>JFSINODE</li>
<li>LPAR</li>
<li>MEM</li>
<li>MEMNEW</li>
<li>MEMUSE</li>
<li>NET</li>
<li>NETERROR</li>
<li>NETPACKET</li>
<li>NFSSVRV2 / NFSSVRV3 / NFSSVRV4 (Automatically extracted but not collected in default config of nmon_helper.sh)</li>
<li>NFSCLIV2 / NFSCLIV3 / NFSCLIV4 (Automatically extracted but not collected in default config of nmon_helper.sh)</li>
<li>PAGE</li>
<li>PROC</li>
<li>PROCSOL</li>
<li>TOP</li>
<li>UARG</li>

</lu>

<br />
<br />


<h2>Technical informations about these system metrics and how they are collected are well described in NMON Analyser Documentation:</h2>

<a href="https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/Power%20Systems/page/nmon_analyser" target="_blank">https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/Power%20Systems/page/nmon_analyser</a>

<br />

<h3>Sarmon site for Solaris version has also a very nice description of NMON Metrics (with some specifics to Sarmon):</h3>

<a href="http://www.geckotechnology.com/fr/sarmon" target="_blank">http://www.geckotechnology.com/fr/sarmon</a>


<br />
<br />


<h2>Host Configuration data (AAA and BBB sections of NMON) can be retrieved as follows:</h2>

<pre>
		index="nmon" sourcetype="nmon_config"
</pre>

<br />
<br />


<h2>Installing NMON (recommended for Linux, optional for Solaris, required for AIX)</h2>

Beginning with Version 1.1.8, NMON App (and Forwarder TA-nmon version) comes with Linux and Solaris (sparc and X86) NMON pre-packages versions.

<br />
<br />

If the "nmon" binary for Linux or "sadc" binary for Solaris is not found within $PATH, then the App will use prepackages versions.
NMON for AIX should be installed in default configuration today, if the nmon binary isn't found in path, an error message will be shown

<br />

You can also (and this recommended for Linux to be sure you have the better NMON version for your distribution) install NMON:

<br />

Installing NMON is out of the scope of this document, here are some links which should help installing NMON for your OS:

<br />
<br />

<h3>AIX NMON Installation:</h3>

<a href="http://www.ibm.com/developerworks/aix/library/au-analyze_aix/" target="_blank">http://www.ibm.com/developerworks/aix/library/au-analyze_aix/</a>

<h3>LINUX NMON Installation:</h3>

For many distributions, NMON shall be available in distrib repository packages (rpm, deb and so on)

<br />
You can also download the last binary for you OS:
<br />

<a href="http://nmon.sourceforge.net/pmwiki.php?n=Site.Download" target="_blank">http://nmon.sourceforge.net/pmwiki.php?n=Site.Download</a>

<h3>SOLARIS NMON (SARMON) Installation:</h3>

Download and installation procedure:
<br />

<a href="http://www.geckotechnology.com/fr/sarmon" target="_blank">http://www.geckotechnology.com/fr/sarmon</a>


<p>
<br />
One great goal of this App is to take huge benefit of Splunk Archive processor system to identify and manage NMON files as it would do with any other standard log file, through a custom archive command stanza
<br />
Splunk call when required the third party script which will convert NMON data in log files Splunk can easily manage.
<br />
<br />
Beyond this, NMON data takes great advantage of Splunk intelligence to exploit this large amount of technical data.
<br />
This Application comes as it is, with absolutely no warranty.
Still i think and hope you will find this very useful and will answer to your needs.
<br />
<br />
Do not hesitate to contact me if you have any further question or comment, any feedback will be greatly appreciated !
<br />
</p>


<h2>WARNING and DISCLAIMER:</h2>

Depending on your nmon command settings, a huge amount of data may be generated by nmon2csv conversion script, don't expect to manage thousands of servers with a free Splunk licence.
<br />	 
<br />

</p>

	 </html>
  </row>
  <row>
    <html>

<div id="PREREQUISITES">

<br />
  	
<h1>2. PREREQUISITES</h1>

</div>

<p>
<h2>Here are requirements for successfully install and use Splunk for NMON</h2>

<br />

<h3>- For TA-nmon (Forwarder App): Python 2.x or Perl standard environment: Depending on your needs, you can choose between the Python converter (default) or the alternative Perl converter</h3>

<br />

<h3>- For Windows OS: Python 2.x environment: The third party script requires a standard and functional Python 2.x environment (see Windows specific instructions, Perl alternative is not supported)</h3>

<br />

<h3>- NMON installation: Optional but recommended for Linux, optional for Solaris and required for AIX (See section below)</h3>

<br />

<h3>- SPLUNK 6.x: I recommend running at least Splunk 6.x for Forwarders (non working situations have been reported with Splunk 5.x) and Splunk 6.1.x for Indexers and Head Searh nodes</h3>

<br />

<br />
Nothing else is required, this App can be used with a free Splunk licence without any limitation, but as said above remember a very large amount of data may have to be indexed.
<br />

<br />  	
  	
</p>

	</html>
  </row>
  <row>
    <html>
	
<div id="INSTALLATION">	

<h1>3. INSTALLATION</h1>

</div>

<br />

<h1>Installation of the Standard NMON for Splunk App: For standalone instance, indexers and Head Search nodes of a Cluster</h1>

<br />

<h2>NMON Splunk App: This is the "normal" and complete Application, the one you downloaded from Splunk Base, or installed through the Splunk Manager</h2>

<h3>The "nmon" App contains:</h3>

<br />

<lu>

<li>Views, Dashboards and Interfaces to exploit and analyse Performance Data of your AIX / Linux / Solaris Systems</li>
<br />

<li>resources for other Topologies: PA-nmon and TA-nmon</li>
<br />

<li>Required inputs, scripts and configuration to get and collect local NMON data on the host where the App has been installed</li>
<br />

</lu>


<h3>Supported Operating System:</h3>

<br />

You can download, install and use the NMON App on any kind of System, Windows, Linux, Unix, Mac OS X...

<br />
<br />

<b>BUT</b>

<br />
<br />

You can ONLY collect local NMON data (performance data for the host that is hosting the App) if your host is running AIX, Solaris or Linux !
<br />
Therefore, you can use the App with Windows / Mac os X to convert any Nmon file or collect nmon Data from Forwarder hosts running the TA-nmon App.
<br />
For Windows, please see specific instructions in the section above.

<br />
<br />

If your are using Universal Forwarders to collect NMON data from various hosts and to send it to your indexer, then this data will be exploitable with any system you've installed the App on.

<br />
<br />

<h2>TA-nmon and PA-nmon: What is it ?</h2>

These are 2 small and specific versions of the standard Application, and are expected to be used as following:

<br />
<br />

<lu>

<li>TA-nmon: This the lightweight version of the NMON App to be installed in Universal Forwarders</li>

<li>PA-nmon: This is the lightweight version of the NMON App to be installed in the master node of a Cluster, then pushed to peer nodes. This is the bundle configuration.</li>

</lu>

<br />

Installation and configuration of these 2 additional App are fully described in the deployment section.

<br />

<div id="STANDARD_INSTALLATION">

<br />

<h2>INSTALLATION:</h2>

</div>

Under normal circumstances, you should use the Splunk Manager to install (and update) the Application.

<br />
<br />

You can do the installation Online (the manager will directly download the package) or download yourself the App and install it with the manager.

<br />
<br />


<b>Example with Online installation:</b>

<br />
<br />

<img src="../../static/app/nmon/various/install_app1.png" alt="Install1"/>

<br />

<br />
<br />

<b>Search for the Nmon Splunk App:</b>

<br />
<br />


<img src="../../static/app/nmon/various/install_app2.png" alt="Install2"/>

<br />
<br />

<b>Once the Application is installed, Please restart Splunk</b>

<br />
<br />

<b>Only if your host is an AIX, Linux or Solaris system, then you can collect local Nmon data by activating inputs:</b>

<br />
<br />


<img src="../../static/app/nmon/various/settings.png" alt="Settings"/>

<br />
<br />




<p>

<h2>Default installation:</h2>

<lu>

<li>
Every NMON data will be indexed into an index called "nmon"
</li>

<li>
In default configuration, the App watches for nmon file available within the directory "/opt/splunk/etc/apps/nmon/nmon_repository"
</li>

<li>
If the Nmon local collect is activated (generating Nmon data with the input script nmon_helper.sh), Nmon files in this directory will be automatically purged by each run of the data collect
</li>

<li>
The directory "/opt/splunk/etc/apps/nmon/spool" will be used as temporary directory by the nmon2csv converter
</li>

<li>
The nmon2scv converter generates csv files within the directory "/opt/splunk/etc/apps/nmon/csv_repository" that will be immediately indexed, and deleted
</li>

</lu>

<br />
<br />


<h2>Additional Monitor:</h2>

You can easily add additional NMON files monitors, such as if you want to monitor other nmon files repository (central shares), this is being described in scenario 3 of Deployment Scenarios section.

<br />
<br />

<h2>Note about indexing CPU cost:</h2>

<br />

Indexing a very large amount of NMON files (moreover if they are large files, 1 day of measure for example) can generate a temporary high system load. (CPU and I/O essentially)

<br />

This is essentially true if you manage NMON files generated outside of Splunk and centralized in a file server.

<br />
<br />

Under normal circumstances (such as NMON data collect through Splunk of the Universal Forwarder), system load is expected to be very trivial.

<br />
<br />

<br />

<div id="WINDOWS_SPECIFIC_INSTRUCTIONS">

<img src="../../static/app/nmon/logos/OS/colors/Windows_72px.png" alt="Windows"/>

<br />
<br />

</div>

<h3>WINDOWS Specific instructions</h3>

<b>Windows OS can be used to Convert / Index and Analyse Nmon Performance Data, therefore a few simple manual steps are required:</b>

<br />
<br />
<b>Install Python 2.x for Windows</b>

<br />
<br />

<lu>

<li>Download and Install Python 2.x package for Windows from: <a href="https://www.python.org/download" target="_blank">https://www.python.org/download</a></li>

</lu>

<br />
<br />

<b>inputs.conf and props.conf</b>

<br />
<br />

<lu>

<li>Copy the inputs.conf_forWindows file from default to local/inputs.conf</li>

<li>Copy the props.conf_forWindows file from default to local/props.conf</li>

<li>Restart Splunk (or refresh using the debug URL: http://server:8000/debug/refresh)</li>

</lu>

<br />
<br />

</p>
	
	</html>
  </row>
  <row>
    <html>

<div id="DEPLOYMENT_SCENARIOS">
	
<h1>4. DEPLOYMENT SCENARIOS</h1>

</div>

<br />

<div id="DEPLOYMENT_SCENARIOS_DISTRIBUTED">

<h1>Scenario 1 "Simple Distributed Environment": Splunk indexer And Splunk Forwarders Agents used to collect Nmon data on servers, Forwarder being deployed by a Splunk Deployment server</h1>

</div>

<h2>In this scenario, A single Splunk indexer will collect NMON Metrics data from clients servers using Splunk Forwarders

<br />
Indexers themselves will collect local NMON Data.</h2>

<img src="../../static/app/nmon/various/diagram_forwarding.png" alt="Diagram"/>


<br />
<br />

<h2>STEP 1: Activate local Nmon data collect in Splunk indexer</h2>

<br />

<p>


<h3>Activate the input Nmon data collect:</h3>

<br />

<img src="../../static/app/nmon/various/settings.png" alt="Settings"/>

<br />
<br />


<br />
<h3>Manually:</h3>
<br />

<pre>
- Copy defaults/inputs.conf to local/, edit the file and look for the adapted nmon_collect entry
</pre>

<br />
<br />
Change "disabled = true" to "false", and restart Splunk.
<br />

</p>


<br />
<h2>STEP 2: Enable Receiving input on the Index Server</h2>

<p>

<br />
Configure the Splunk Index Server to receive data, either in the manager:

<br />
<br />

<pre>
Manager -&gt; sending and receiving -&gt; configure receiving -&gt; new 
</pre>

<br />
<br />
or via the CLI:
<br />
<br />

<pre>
		/opt/splunk/bin/splunk enable listen 9997 
</pre>

<br />
<br />
Where 9997 (default) is the receiving port for Splunk Forwarder connections
<br />


</p>



<br />
<h2>STEP 3: Prepare your package to be deployed</h2>

<br />

<p>

<h4>First, extract the content of TA-nmon App provided in $SPLUNK/etc/apps/nmon/resources to $SPLUNK_HOME/etc/deployment-apps, example:</h4>

<br />

<pre>
cd /opt/splunk/etc/deployment-apps

tar -xvzf /opt/splunk/etc/apps/nmon/resources/TA-nmon*.tar.gz
</pre>


<br />


<br />

<h4>Then, configure the outputs.conf to enable communication between the Forwarder and your Splunk server, example:</h4>

<br />

edit /opt/splunk/etc/deployment-apps/TA-nmon/local/outputs.conf

<br />
<br />

<h4>Example of simple configuration:</h4>

<br />

<pre>

[tcpout]
defaultGroup = default-autolb-group

[tcpout:default-autolb-group]
server = mysplunk-server:9997

[tcpout-server://mysplunk-server:9997]

</pre>




<br />
<h2>STEP 4: Activate and Configure the Splunk Deployment server</h2>

<p>

<br />

<h4>Configure the Splunk Server to act as a Deployment Server:</h4>

<br />
In CLI:
<br />
<br />

<pre>
/opt/splunk/bin/splunk enable deploy-server -auth admin 
</pre>

<br />

<br />
Configure your serverclass.conf file, the following simple example will filter your hosts based on their hostname, and automatically deploy the TA-nmon App:

<br />
<br />
Edit the file "$SPLUNK_HOME/etc/system/local/serverclass.conf" with section:
<br />

<pre>
[serverClass:linux_hosts]
whitelist.0 = *linux*

[serverClass:solaris_hosts]
whitelist.0 = *solaris*

[serverClass:solaris_hosts:app:TA-nmon]
restartSplunkWeb = 0
restartSplunkd = 1
stateOnClient = enabled

[serverClass:linux_hosts:app:TA-nmon]
restartSplunkWeb = 0
restartSplunkd = 1
stateOnClient = enabled
</pre>

<br />

<br />
Restart your Splunk server.
<br />
<br />


</p>





<h4>Go in the "Forwarder Management" page in Splunk Manager (distributed environment)</h4>


<br />
You should see 2 Applications and Classes:

<br />
<br />
<img src="../../static/app/nmon/various/splunk_forwarder_manage1.png" alt="Settings1"/>

<br />
<br />

<img src="../../static/app/nmon/various/splunk_forwarder_manage2.png" alt="Settings2"/>

<br />
<br />


<h4>The Splunk server is now ready to act as Deployment server, next steps will concern client installation and initial configuration.</h4>


</p>


<br />
<br />

<h2>STEP 5: Clients Forwarders Installation and configuration</h2>

<p>

<br />
Note: If forwarders are already installed and connected to your deployment server, you can off course bypass this section

<br />
<br />

Steps for Installing/Configuring *nix forwarders:

</p>

<br />
<h4>2.1 Download Splunk Universal Forwarder:</h4>

<br />

<p>

<a href="http://www.splunk.com/download/universalforwarder" target="_blank">http://www.splunk.com/download/universalforwarder</a>

</p>

<br />
<h4>2.2 Install Forwarder</h4>

<br />
<h4>2.3 Enable boot-start/init script:</h4>

<p>

<br />
Activate the forwarder at boot time:
<br />
<br />

<pre>
/opt/splunkforwarder/bin/splunk enable boot-start
</pre>

<br />
<br />
To start the forwarder:
<br />
<br />

<pre>
/opt/splunkforwarder/bin/splunk start
</pre>

</p>

<br />
<h4>2.4 Connect the Forwarder to your Server:</h4>

<p>

<br />

On Forwarders:

<br />


<br />
On the deployment client, run these CLI commands:
<br />
<br />


<pre>
splunk set deploy-poll &lt;IP_address/hostname&gt;:&lt;management_port&gt;
</pre>

<br />
<br />

<pre>
splunk restart
</pre>

<br />

<br />
Use the IP_address/hostname and management_port of the deployment server you want the client to connect with.

<br />
<br />

For example:

<br />
<br />

<pre>
splunk set deploy-poll deploymentserver.splunk.mycompany.com:8089

splunk restart
</pre>




</p>



<br />
<br />

<h2>FINAL: Check your clients deployments:</h2>

<p>

<br />

After your restarted the client (upon initial configuration above), wait a few minutes (be patient, this can require time) and if everything is ok you will quickly see the application being deployed within your client.

<br />

Congratulations :-)

<br />
<br />

<img src="../../static/app/nmon/various/splunk_forwarder_manage3.png" alt="Settings3"/>


</p>



<br />
<br />


<br />

<div id="DEPLOYMENT_SCENARIOS_CLUSTER">

<h1>Scenario 2 "Clustering Distributed Environment": Monitor node members of Splunk clusters with Nmon Performance Application</h1>

</div>

<br />

<img src="../../static/app/nmon/various/Basic_cluster_60.png" alt="Basic Cluster"/>

<br />
<br />


<h2>
This scenario is a full Cluster and Clients Splunk implementation taking the benefit of Nmon Performance Monitor, things will work as follows:</h2>

<br />

<p>
<b>Please note the following configuration can be adapted to whatever you need or prefer.</b>
</p>

<br />

<lu>

<li>You have a functional Splunk cluster running: master node, peers nodes, heads searches and optionally a deployment server</li>
<br />

<li>The "PA-nmon" Application will be uploaded to the master node, then pushed to peer nodes using Splunk bundle configuration standard mechanism</li>
<br />

<li>Upon bundle configuration update, each peer node will automatically generate and collect Nmon data to the replicated nmon Index</li>
<br />

<li>The core Application (nmon) which contains graphical interfaces will be installed in search head nodes, it will also generate Nmon performance data for head search</li>
<br />

<li>The "TA-nmon" will be deployed to the master node and other nodes (deployment servers...) to generate Nmon performance data</li>
<br />

</lu>

<br />


<br />
<h2>Let's start.</h2>

<br />

<h3>For the tutorial purposes, i will assume we have:</h3>

<br />

<lu>

<li>
          <b>A Splunk master node:</b> splunk-master</li>
<br />

<li>
          <b>Peers node:</b> splunk-peer1, splunk-peer2, splunk-peer3</li>
<br />

<li>
          <b>3 search head nodes:</b> splunk-head1, splunk-head2, splunk-head3</li>
<br />

<li>
          <b>A Splunk deployment instance:</b> splunk-deployment</li>

</lu>


<br />
<br />
<br />

<h2>STEP 1: Enable Receiving input on each peers node</h2>

<p>

<br />
<h3>Each peer node must be ready to receive data:</h3>

<br />
<br />
Using Splunk Web:
<br />
<br />

<pre>
Manager -&gt; sending and receiving -&gt; configure receiving -&gt; new 
</pre>

<br />
<br />
or via the CLI:
<br />
<br />

<pre>
/opt/splunk/bin/splunk enable listen 9997 
</pre>

<br />
<br />
Where 9997 (default) is the receiving port for Splunk Forwarder connections
<br />


</p>

<br />
<br />

<h2>STEP 2: Ensure all nodes are set in Forwarding mode</h2>

<p>

<br />

<h3>
As Splunk best practice recommends, you need to ensure all your cluster nodes (search heads, master node, deployment servers...) are set to forward
data to the Cluster
</h3>

<br />

<b>For more information, please read:</b>

<br />
<br />

<lu>

<li>

<a href="http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/Forwardsearchheaddata" target="_blank">http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/Forwardsearchheaddata</a> 

</li>

<br />

<li>

<a href="http://docs.splunk.com/Documentation/Splunk/latest/Indexer/Forwardmasterdata" target="_blank">http://docs.splunk.com/Documentation/Splunk/latest/Indexer/Forwardmasterdata</a>

</li>

</lu>

<br />
<br />

Achieving this is very simple, in every required node, create an outputs.conf file with contains the indexer pool, example:

<br />
<br />

<pre>

$SPLUNK_HOME/etc/system/local/outputs.conf


# Turn off indexing on the search head
[indexAndForward]
index = false
 
[tcpout]
defaultGroup = my_search_peers 
forwardedindex.filter.disable = true  
indexAndForward = false 
 
[tcpout:my_search_peers]
server=splunk-peer1:9997,splunk-peer2:9997,splunk-peer3:9997
autoLB = true


</pre>

<br />

Then restart Splunk.

</p>


<br />
<br />

<h2>STEP 3: Upload and extract the PA-nmon (bundle configuration) to the master node</h2>

<br />

<p>

<h3>Download the NMON Splunk App, extract its content and upload the "PA-nmon" App archive available within "resources" directory. (to /tmp directory for example)</h3>

<br />

<pre>


nmon/resources/PA_nmon_Vx.x.xx.tar.gz

</pre>

<br />

<h3>Extract the PA-nmon App to the master-apps directory in the master mode, example:</h3>

<br />

<pre>


cd /opt/splunk/etc/master-apps

tar -xvzf /tmp/PA-nmon_V*.tar.gz

</pre>

</p>

<br />

<h2>STEP 4: Deploy the bundle configuration from to the master node to your peer nodes</h2>

<p>

<br />

<h3>In master node, Push the configuration to your peers using the CLI:</h3>

<br />

<pre>


splunk apply cluster-bundle

splunk show cluster-bundle-status

</pre>

<br />

<h3>After all peers have restarted, you should see a new index replicated within the master dashboard (be patient and wait a few minutes before your can see your new replicated index):</h3>

<br />
<br />

<img src="../../static/app/nmon/various/cluster1.png" alt="Cluster1"/>

<br />

</p>


<br />
<br />


<h2>STEP 5: Deploy the core Application to head search nodes</h2>

<br />

<h3>On every search head, you need to deploy the core Application "nmon" which contains graphical views and interfaces, and will also generates the Nmon performance data for these nodes</h3>

<br />

<h3>Search Head Clustering: Starting with Splunk 6.2</h3>

<br />

If you are using the Search Head clustering mechanism introduced with version 6.2:

<br />
<br />

NOTE: For more information about search head clustering and application deployment, see: http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/PropagateSHCconfigurationchanges

<br />
<br />

On the deployer, the configuration bundle resides under the $SPLUNK_HOME/etc/shcluster directory. The set of files under that directory constitutes the configuration bundle.

<br />

The directory has this structure:

<br />
<br />

<pre>


$SPLUNK_HOME/etc/shcluster/
     apps/
          &lt;app-name&gt;/
          &lt;app-name&gt;/
          ...
     users/

</pre>

<br />

Extract the content of the core Application (the tar archive you downloaded from Splunk base) to the "apps" directory.

<br />
<br />

Then, activate the nmon_helper.sh to collect Nmon Performance data:

<br />
<br />

<b>Create the local inputs.conf file in "nmon/local/inputs.conf" wich activates the helper script:</b>

<br />
<br />

<pre>
$SPLUNK_HOME/etc/shcluster/apps/nmon/local/inputs.conf

[script://./bin/nmon_helper.sh]
disabled = false

</pre>

<br />

<b>And finally push the configuration bundle:</b>

<br />
<br />

<pre>


splunk apply shcluster-bundle -target &lt;URI&gt;:&lt;management_port&gt; -auth &lt;username&gt;:&lt;password&gt;


</pre>

<br />
<br />

<h3>For Non clustered head search:</h3>

<br />

Extract the content of the core Application (the tar archive you downloaded from Splunk base) to the "apps" directory of the search head

<br />
<br />

Then, activate the nmon_helper.sh to collect Nmon Performance data:

<br />
<br />

<b>Create the local inputs.conf file in "nmon/local/inputs.conf" wich activates the helper script:</b>

<br />
<br />

<pre>

$SPLUNK_HOME/etc/apps/local/inputs.conf

[script://./bin/nmon_helper.sh]
disabled = false

</pre>

<br />

And restart the head search instance


<br />
<br />


<h2>STEP 6: Other type of instance such as a deployment server</h2>

<br />

<h3>Every other type of instance should use the "TA-nmon" Application to forward the Nmon performance data to the cluster.</h3>

In the same way, ensure that the instance has been configured to forward every local data to the cluster.

<br />
<br />


<h3>Upload the "TA-nmon" App archive available within "resources" directory. (to /tmp directory for example)</h3>

<br />

<pre>


nmon/resources/TA_nmon_Vx.x.xx.tar.gz

</pre>

<br />

<h3>Extract the TA-nmon App to the apps directory of the instance, example:</h3>

<br />

<pre>


cd /opt/splunk/etc/apps

tar -xvzf /tmp/TA-nmon_V*.tar.gz

</pre>



<br />
<br />


<h2>FINAL: Access your data</h2>

<p>

<br />

<b>Every member of your Splunk Cluster has now NMON Performance Data being collected, and searchable over all your Cluster! Great!</b>

<br />
<br />

<img src="../../static/app/nmon/various/cluster_search1.png" alt="Cluster_Search1"/>

<br />
<br />

<img src="../../static/app/nmon/various/cluster_search2.png" alt="Cluster_Search2"/>

<br />
<br />



</p>

<div id="DEPLOYMENT_SCENARIOS_CENTRAL">

<h1>Scenario 3: Manage NMON Data collected into centralized shares (Applicable for Indexers / Heavy Forwarders / Light Forwarders)</h1>

</div>

<h2>
In a scenario where there is no Splunk forwarders installed in servers but there is another process in place which periodically collect Nmon data, all you need is a central share (such as an NFS share)
which Splunk indexer has access.</h2>

<br />

<h2>CAUTION: The App won't manage nmon files being currently updated by the nmon binary running, if you give Splunk an access to nmon files on running this will generate duplicated events each time files are updated !
</h2>
<h2>This scenario intends to manage cold nmon files only.</h2>

<br />

<h2>STEP 1: Splunk indexer Nmon metrics local collect</h2>

<p>

<br />

In such a scenario, you will still probably want to have Splunk indexer metrics being collected locally, to do so:
<br />
<br />


<h3>Activate the input accorded to your local OS type:</h3>

<br />

<img src="../../static/app/nmon/various/settings.png" alt="Settings"/>

<br />
<br />


<br />
<h3>Manually:</h3>
<br />

<pre>
- Copy defaults/inputs.conf to local/, edit the file and look for the adapted nmon_collect entry
</pre>

<br />
<br />
Change "disabled = true" to "false", and restart Splunk.
<br />

</p>

<br />
<br />

<h2>STEP 2: Add an additional monitor that watches for files and directories, and finally convert/index nmon files </h2>

<p>

<br />
So you have a central file server share where Nmon files are being centralized, the better option is probably having it mounted using NFS.

<br />
<br />

<b>For the example, we will admit nmon files are being stored in a time stamped directory for each day:</b>

<pre>
/mnt/NFS-SHARE/nmon-repository/YYYY-MM-DD/*.nmon
</pre>

<br />

<b>All you need is creating a custom monitor for your repository, this can be set using Splunk Web or Manually:</b>

<br />
<br />
<br />

<h3>
Using Splunk Web (except for light forwarder):
</h3>

<br />
<b>INFORMATION:</b> I recommend to set the crcSalft setting which will ensure Splunk won't ignore Nmon files with a very small size, or data too much similar.
<br />
This cannot be done with the manager and me be set manually (can be added in the inputs.conf file after the operation above)

<br />
<br />

<pre>
crcSalt = &lt;SOURCE&gt;
</pre>

<br />
<br />

<img src="../../static/app/nmon/various/custom_repo_splunkweb1.png" alt="custom_repo_splunkweb1"/>

<br />
<br />

<img src="../../static/app/nmon/various/custom_repo_splunkweb2.png" alt="custom_repo_splunkweb2"/>

<br />
<br />

<img src="../../static/app/nmon/various/custom_repo_splunkweb3.png" alt="custom_repo_splunkweb3"/>

<br />
<br />

<img src="../../static/app/nmon/various/custom_repo_splunkweb4.png" alt="custom_repo_splunkweb4"/>

<br />
<br />

<img src="../../static/app/nmon/various/custom_repo_splunkweb5.png" alt="custom_repo_splunkweb5"/>



<br />
<br />

<h3>
Manually:
</h3>

<br />
<br />

<b>INFO:</b> Since Version 1.5.07, you can manage gzip compressed files without further more configuration, just replace *.nmon by *.nmon.gz

<br />
<br />

<pre>
[monitor:///mnt/NFS-SHARE/nmon-repository/*/*nmon]
disabled = false
index = nmon
sourcetype = nmon_processing
crcSalt = &lt;SOURCE&gt;
</pre>

<br />

<b>Or an alternative version using whitelist and manage any nmon files in sub folders:</b>

<br />

<pre>
[monitor:///mnt/NFS-SHARE/nmon-repository/]
disabled = false
whitelist = \.nmon$
index = nmon
sourcetype = nmon_processing
crcSalt = &lt;SOURCE&gt;
</pre>

<b>And even:</b>

<br />

<pre>
[monitor:///mnt/NFS-SHARE/nmon-repository/.../*.nmon]
disabled = false
index = nmon
sourcetype = nmon_processing
crcSalt = &lt;SOURCE&gt;
</pre>


<br />
<br />


	

<b>Then restart Splunk. (or refresh the configuration using the debug URL: http://myserver:8000/debug/refresh)</b>

<br />
<br />	

<b>Depending on the number of files and directory, Splunk may requires some times before beginning to start files conversion and indexing.</b>

<br />
<br />	

NOTE: Keep in mind that if many files are present, an heavy and long process of conversion and indexing data will have to done, which can also result in a licence violation because a massive indexing operation. (thus that's not necessary a problem)

<br />
<br />	


</p>

<div id="DEPLOYMENT_SCENARIOS_CENTRAL_CLUSTER">

<h1>Scenario 4: Manage NMON Data collected into centralized shares in a Splunk Cluster environment, using Forwarders to convert and stream Nmon Data</h1>

</div>

<h2>
This deployment scenario intends to manage Nmon in central shares in a Splunk cluster environment, a Splunk Forwarder instance (either light or heavy) will be used to convert and stream the nmon data to the Splunk cluster.
</h2>

<br />

<h2>CAUTION: The App won't manage nmon files being currently updated by the nmon binary running, if you give Splunk an access to nmon files on running this will generate duplicated events each time files are updated !
</h2>
<h2>This scenario intends to manage cold nmon files only.</h2>

<br />

<h2>STEP 1: Deploy the PA-nmon (bundle configuration) to the master node</h2>

<br />

<p>

Download the NMON Splunk App, extract its content and upload the "PA-nmon" App archive available within "resources" directory. (to /tmp directory for example)

<br />
<br />

Then, extract the PA-nmon App to the master-apps directory in the master mode, example:

<br />
<br />


<pre>cd /opt/splunk/etc/master-apps</pre>

<br />
<br />

<pre>tar -xvzf /tmp/PA-nmon_V*.tar.gz</pre>

<br />
<br />

<b>INFORMATION: By default, the PA-nmon will generate local nmon data for peer nodes, if you don't want to get performance data using the App, you can deactivate this feature as follows:</b>

<br />
<br />

Create a local inputs.conf which will disable the nmon_helper.sh input script:

<br />
<br />

<b>$SPLUNK_HOME/etc/master-apps/PA-nmon/local/inputs.conf</b>

<pre>
[script://./bin/nmon_helper.sh]
disabled = true
</pre>

</p>

<br />

<h2>STEP 2: Deploy bundle configuration from to the master node to your peer nodes</h2>

<p>

<br />

In master node, Push the configuration to your peers:

<br />
<br />

<pre>splunk apply cluster-bundle</pre>

<br />
<br />

<pre>splunk show cluster-bundle-status</pre>

<br />
<br />

After all peers have restarted, you should see a new index replicated within the master dashboard (be patient and wait a few minutes before your can see your new replicated index) :

<br />
<br />

<img src="../../static/app/nmon/various/cluster1.png" alt="Cluster1"/>

<br />

</p>


<br />

<h2>STEP 3: Enable Receiving input on each peers node</h2>

<p>

<br />
Configure each peer node to receive data, either in the manager:

<br />
<br />

<pre>
Manager -&gt; sending and receiving -&gt; configure receiving -&gt; new 
</pre>

<br />
<br />
or via the CLI:
<br />
<br />

<pre>
/opt/splunk/bin/splunk enable listen 9997 
</pre>

<br />
<br />
Where 9997 (default) is the receiving port for Splunk Forwarder connections
<br />


</p>

<br />

<h2>STEP 4: Install the TA-nmon in your Forwarder and configure the custom input</h2>

<br />

<p>

For the example, we will assume a manual installation of the TA-nmon App, and we will assume the Forwarder is already connected to the cluster (outputs.conf)

<br />

If you intend to deploy the TA-nmon App using the deployment manager, or have to configure the connection with the cluster, please refer to scenarios 1 and 2.

<br />
<br />

Upload the "TA-nmon" App archive available within "resources" directory to your Splunk Forwarder (light or heavy). (to /tmp directory for example)

<br />
<br />

Then, extract the TA-nmon App in the Splunk application directory, example with a light forwarder:

<br />
<br />


<pre>cd /opt/splunkforwarder/etc/apps</pre>

<br />
<br />

<pre>tar -xvzf /tmp/TA-nmon_V*.tar.gz</pre>

<br />
<br />

<b>INFORMATION: By default, the TA-nmon will generate local nmon data for the host running the App, if you don't want to get performance data, you can deactivate this feature as follows:</b>

<br />
<br />

Create a local inputs.conf which will disable the nmon_helper.sh input script:

<br />
<br />

<b>$SPLUNK_HOME/etc/master-apps/TA-nmon/local/inputs.conf</b>

<pre>
[script://./bin/nmon_helper.sh]
disabled = true
</pre>



<br />

Adding a custom Nmon input for your central share is very easy, here is an example:

<br />
<br />

<b>INFO:</b> Since Version 1.5.07, you can manage gzip compressed files without further more configuration, just replace *.nmon by *.nmon.gz

<br />
<br />

<b>Create "$SPLUNK_HOME/etc/apps/nmon/local/inputs.conf" with the following configuration:</b>

<br />

<pre>
[monitor:///mnt/NFS-SHARE/nmon-repository/*/*nmon]
disabled = false
index = nmon
sourcetype = nmon_processing
crcSalt = &lt;SOURCE&gt;
</pre>

<br />

<b>Or an alternative version using whitelist and manage any nmon files in sub folders:</b>

<br />

<pre>
[monitor:///mnt/NFS-SHARE/nmon-repository/]
disabled = false
whitelist = \.nmon$
index = nmon
sourcetype = nmon_processing
crcSalt = &lt;SOURCE&gt;
</pre>

<b>And even:</b>

<br />

<pre>
[monitor:///mnt/NFS-SHARE/nmon-repository/.../*.nmon]
disabled = false
index = nmon
sourcetype = nmon_processing
crcSalt = &lt;SOURCE&gt;
</pre>

<br />
<br />	

Finally, restart the Splunk Forwarder.

<br />
<br />

Immediately after restart, the App will begin to proceed to any Nmon file located in the central share, and will start to stream the data to the Splunk Cluster

<br />
<br />

</p>

<br />





	
	</html>
  </row>
  <row>
    <html>
	
<div id="ADVANCED_CONFIGURATION">
	
<h1>5. ADVANCED CONFIGURATION</h1>

</div>

<div id="ADVANCED_ACCURACY">

<br />

<h2>5.1 Configure NMON using the "nmon.conf" configuration file: Use provided modes, set custom interval and snapshot</h2>

</div>

<br />


<b>Since the Version 1.5.0, it is possible to customize the way NMON data is being generated by configuring the "nmon.conf" file</b>

<br />
<br />

The "nmon.conf" file is located in the "default" directory of nmon / TA-nmon / PA-nmon Apps, it works in a "Splunk" fashion, to modify these settings, copy the default/non.conf to local/nmon.conf. (upgrade resilient)

<br />
This configuration file is sourced at starting time by the "nmon_helper.sh" input script which apply these settings.

<br />
<br />


<b>A number of preset configuration are provided for proposal:</b>

<br />
<br />

<b>shortperiod_low</b>

<pre>
#	shortperiod_low)
#			interval="60"
#			snapshot="10"
</pre>

<b>shortperiod_middle</b>

<pre>
#	shortperiod_middle)
#			interval="30"
#			snapshot="20"
</pre>

<b>shortperiod_high</b>

<pre>	
#	shortperiod_high)
#			interval="20"
#			snapshot="30"
</pre>

<b>longperiod_low</b>

<pre>
#	longperiod_low)
#			interval="240"
#			snapshot="120"
</pre>

<b>longperiod_middle</b>

<pre>
#	longperiod_middle)
#			interval="120"
#			snapshot="120"
</pre>

<b>longperiod_high</b>

<pre>
#	longperiod_high)
#			interval="60"
#			snapshot="120"
</pre>


<br />

<b>The default mode is set to "longperiod_high", which is a good compromise between accuracy, CPU / licensing cost and operational intelligence, and should be relevant for very large deployment in Production environments</b>

<br />
<br />

<b>A custom mode combining custom values for snapshot and interval can be set:</b>

<br />

<pre>

# custom --> Set a custom interval and snapshot value, if unset short default values will be used (see custom_interval and custom_snapshot)

# Default is longperiod_low
mode="longperiod_low"

# Refresh interval in seconds, Nmon will this value to refresh data each X seconds
# UNUSED IF NOT SET TO custom MODE
custom_interval="20"

# Number of Data refresh occurrences, Nmon will refresh data X times
# UNUSED IF NOT SET TO custom MODE
custom_snapshot="90"

</pre>

<br />

<b>Note that the CPU usage associated with Nmon and Splunk processing steps, the data volume to be generated and the licensing cost are a combination of these factors</b>

<br />
<br />

<div id="ACTIVATE_NFS">

<br />
<br />

<h2>5.2 Activate the generation of NFS Statistics:</h2>

<br />

<b>Since Version 1.5.0, Activating the generation of NFS statistics is controlled the "nmon.conf" configuration file. (see section above)</b>

<br />
<br />

<b>To activate NFS statistics, configure your "local/nmon.conf" file:</b>

<br />
<br />

<pre>

### NFS OPTIONS ###

# Change to "1" to activate NFS V2 / V3 (option -N) for AIX hosts
AIX_NFS23="0"

# Change to "1" to activate NFS V4 (option -NN) for AIX hosts
AIX_NFS4="0"

# Change to "1" to activate NFS V2 / V3 / V4 (option -N) for Linux hosts
# Note: Some versions of Nmon introduced a bug that makes Nmon to core when activating NFS, ensure your version is not outdated
Linux_NFS="0"

</pre>

<br />
<br />

<b>Take note that some older Nmon releases in Linux introduced a bug that was causing Nmon process to core when activating NFS statistics, in case of trouble please ensure you are not affected by this</b>


</div>


<div id="ADVANCED_FRAMEID">

<br />

<h2>5.3 FRAME ID: Mapping hostnames with a Frame Identifier</h2>

</div>

<br />

<p>

<b>
In large deployment scenarios, mapping hostnames with their Frame Identifier can be very useful to help Analysis, or simply finding the required host.

<br />
Since Version 1.5.0, a Frame ID feature is included within interfaces, in default configuration the frame ID is mapped to the Serial Number of the host.

<br />
In AIX OS, the Serial Number is associated the PSeries Serial Number (in Pseries environments), in Linux / Solaris, this is equal to the hostname.

<br />
<br />

You can customize the Frame Identifier using any external lookup table which will contains one field for the frameIDs, and one field containing hostnames.

<br />
To achieve this, please follow the configuration above:
<br />

</b>


<br />
<br />

<b>1. Configure your table lookup in transforms.conf</b>

<br />
<br />

Create a local/transforms.conf and set your lookup table:

<br />

<pre>

[myframeidtable]
filename = my_frameid_lookup.csv
</pre>

<br />

<b>Example 1: Map Pseries with hostnames using the serial number field</b>

<br />

<pre>
PSERIES_NAME,serialnum
PSERIESfoo,xxxxxxxxxxx
PSERIESbar,xxxxxxxxxxx
</pre>

<br />

<b>Example 2: Map frameID with hostnames (using the hostname field)</b>

<br />

<pre>
FRAME_NAME,hostname
frame1,hostname1
frame1,hostname2
frame2,hostname3
frame3,hostname4
</pre>

<br />

<b>2. Map your hostnames with the frameID in props.conf</b>

<br />
<br />

Create a local/props.conf and map your hosts within the nmon_data stanza:

<br />
<br />

<b>Example 1: (Pseries with serial number field)</b>

<pre>
[nmon_data]
LOOKUP-myframeidtable = myframeidtable serialnum AS serialnum OUTPUT PSERIES AS frameID
</pre>

<br />

<b>Example 2: (frameID with hostnames)</b>

<pre>
[nmon_data]
LOOKUP-myframeidtable = myframeidtable hostname OUTPUT FRAME_NAME AS frameID
</pre>

<br />
<br />

NOTE: Use "OUTPUT" to generate the frameID field, don't use OUTPUTNEW which wont't overwrite the default frameID field

<br />
<br />

<b>3. Restart Splunk to apply settings</b>

<br />
<br />

<b>4. Rebuild Acceleration for Datamodel</b>

<br />
<br />

For each accelerated Data model, please rebuild the acceleration to update the frameID field. (Go in Pivot, manage datamodels, develop each data model and rebuild)

<br />
<br />

</p>


<div id="ADVANCED_SPAN">

<br />
<br />

<h2>5.4 Time Interval definition: Custom macros used by App to dynamically define the more accurate span value</h2>

</div>

<br />

<p>

<b>
*** SINCE VERSION 1.4.4, this step is not required any more, Proceed to this step ONLY if you have data indexed prior to Version 1.4.4 ***
</b>

<br />
<br />

<b>
NMON Splunk App uses an advanced search (eg. macro) to dynamically define the more accurate interval time definition possible within charts.
</b>

<br />
<br />

Splunk has a charting limit of 1000 points per series, an adapted span value (time interval) has to be defined if we want charts to be more accurate than Splunk automatically affects

<br />
<br />

This is why this custom macro is being defined based on analysing Time ranges supplied by users, see:

</p>


<pre>
${SPLUNK_HOME}/etc/apps/nmon/default/macros.conf
</pre>

<br />
<br />


<p>

If you have a different minimal time interval than 10 seconds between 2 measures at the lower level, you can customize these macro to adapt them to your data. (as for an example if you generate NMON data with an other process than Splunk)

<br />

Simply copy macros.conf to your local/ directory and issue your modifications, please note a 5 minute time interval macro example if provided within configuration file.

</p>

<br />
<br />


	</html>
  </row>
  <row>
    <html>

<div id="USAGE">

<h1>6. USAGE</h1>

</div>

<div id="USAGE_COLLECT">

<br />

<h2>6.1 NMON Data Collect (using the App to generate nmon files):</h2>

</div>

You can use the Application to generate nmon files that will be converted and indexed within Splunk, this is called the Data Collect operation.

<br />
<br />

<b>
The Data Collect is being operated by the input script "nmon_helper.sh" (for nmon / TA-nmon / PA-nmon), its output processing itself is indexed within Splunk:
</b>

<br />
<br />

<pre>
		index="nmon" sourcetype="nmon_collect"
</pre>		

<br />
<br />

<b>
You can also access to these information through pre-configured reports:
</b>

<br />

<img src="../../static/app/nmon/various/reports_collect.png" alt="reports_collect"/>

<br />

<div id="USAGE_PROCESSING">

<br />

<h2>6.2 NMON Data Processing (conversion of Nmon raw data):</h2>

</div>

Whenever you are using the Application to collect the nmon data (generating nmon files) or only using the App to manage existing nmon files (generated by a third party workflow), Splunk watches for
directories (the default repository or custom repositories) and when it finds a files that has not been yet handled, it proceeds to its conversion.

<br />
<br />

<b>
This is achieved by the nmon2csv converter Python script (nmon2csv.py) or its Perl alternative (nmon2csv.pl), its output contains many useful information about the conversion step, it can be retrieved by:
</b>

<br />
<br />

<pre>
		index="nmon" sourcetype="nmon_processing"
</pre>		

<br />
<br />

<b>
You can also access to these information through pre-configured reports:
</b>

<br />
<br />

<img src="../../static/app/nmon/various/reports_processing.png" alt="reports_processing"/>

<br />
<br />

<b>
The "Application Internal Statistics" interface available from Home page provides a graphical analysis of these data:
</b>

<br />
<br />

<img src="../../static/app/nmon/various/application_internal_stats_processing.png" alt="application_internal_stats_processing"/>

<br />

<div id="USAGE_DATA">

<br />

<h2>6.3 NMON Performance Monitor Data:</h2>

</div>

<b>Once indexed, the Nmon Performance Data can be easily retrieved:</b>

<br />
<br />

<pre>
		index="nmon" sourcetype="nmon_data"
</pre>		

<br />
<br />

This is the way all interfaces will access to the Performance Data, the type of Performance Monitor (CPU % usage, % of time busy disk...) can be filtered using the "type" field.

<br />
<br />

Fields that are available within Splunk are dependent of the type of Performance monitor, each "type" corresponds to a csv file previously indexed by Splunk.

<br />

These fields are automatically generated during Nmon Processing steps and may differ between Operating Systems.

<br />
<br />

<b>For example, to retrieve CPU percentage Usage, use:</b>

<br />
<br />

<pre>
		index="nmon" sourcetype="nmon_data" type="CPU_ALL"
</pre>		

<br />
<br />



<br />
<br />

	</html>
  </row>
  <row>
    <html>	

<div id="UPGRADE_INSTRUCTIONS">

<br />

<h1>7. UPGRADE INSTRUCTIONS</h1>

</div>

<br />

<h2>7.1 Upgrade "nmon" standard Application for Indexers and Head search nodes</h2>

<p>

Upgrading Splunk for NMON App is easy as for any other Splunk App, just upgrade the App through the manager and you're done. (this can also be done manually but i would recommend using the Manager)

<br />

Restarting Splunk after update is recommended.

<br />
<br />
Note that any configuration file located in "local" directory will not be affected by the update procedure. (Splunk standard)

<br />
<br />
Therefore, as with any upgrade or update operation, i strongly recommend to have up to date backups before trying any update, moreover on Production systems.

</p>

<br />

<h2>7.2 Upgrade of TA-nmon</h2>

<p>

When using Splunk Deployment server (see scenario 1), upgrading the TA-nmon Application will be achieved very easily by extracting the new TA-nmon archive version, note that local configuration will never be touched or overwritten

<lu>

<li>In CLI, go within the deployment directory and extract the new TA-nmon version: (that you uploaded in /tmp for example)</li>

<br />

<pre>
$ cd /opt/splunk/etc/deployment-apps
</pre>

<br />
<br />

<pre>
$ tar -xvzf /tmp/TA-nmon*.tar.gz
</pre>

<br />
<br />

<li>You can ask Splunk to reload the Deployment Server, It will then start to upgrade any connected client</li>

<br />

<pre>
$ $SPLUNK_HOME/bin/splunk reload deploy-server
</pre>

<br />
<br />

</lu>

<br />

</p>


<h2>7.3 Upgrade of PA-nmon for Splunk Clusters (bundle configuration to deploy to master node, and then to be pushed to peer nodes)</h2>

<p>

<lu>

<li>In CLI, go within the master node and extract the new PA-nmon version: (that you uploaded in /tmp for example)</li>

<br />

<pre>
$ cd /opt/splunk/etc/master-apps
</pre>

<br />
<br />

<pre>
$ tar -xvzf /tmp/PA-nmon*.tar.gz
</pre>

<br />
<br />

<li>In CLI, Apply the new bundle configuration:</li>

<br />

<pre>
$ splunk apply cluster-bundle
</pre>

<br />
<br />

<pre>
$ splunk show cluster-bundle-status
</pre>


<br />
<br />

</lu>

<br />

</p>





<br />

	</html>
  </row>
  <row>
    <html>

<div id="REFERENCE_MATERIAL">

<h1>8. REFERENCE MATERIAL</h1>

</div>

<br />

<p>

<lu>

<li>
<b>nmon2csv.py / nmon2csv.pl:</b> 
</li>

<br />
Formerly nmon2csv, these are Nmon converters which will translate Nmon structured data into data Splunk will index and be able to manage.

<br />
In default configuration, the Python version will be used, the Perl version is provided as an alternative for system lacking a Python core environment, and where installing
it may be a challenge... (not necessary technically !)

<br />
Both Python and Perl versions only uses core modules, such that they can be executed without having to install anything specifically.

<br />
These script are automatically Invoked by the Splunk Archive Processor whenever required, and reads data from standard input (stdin). (see props.conf)

<br />
<br />

<li>
<b>nmon_helper.sh:</b>
</li>

<br />

This is a third party Shell script used to collect the NMON performance data for AIX / Linux / Solaris hosts.

<br />
Written in sh, it does not have any specific requirements and should work with any Operating System.

<br />
At starting time, the script will operate a few verifications and will start the Nmon binary if required.

<br />
Since the version 1.5.0 of the Nmon App, main Nmon options can be controlled by the nmon.conf file (located in default directory), for customization purposes you can create your own version of the nmon.conf file in
your local directory, changes will be permanent and upgrade resilient.


<br />
<br />

<li>
<b>nmon_cleaner.sh / nmon_cleaner.py / nmon_cleaner.pl:</b> 
</li>

<br />

These scripts will be used by the App to manage the retention of raw Nmon data files.
<br />
In default configuration, raw Nmon files will be locally kept during 7 days.
<br />
Note that these scripts can be used to ensure csv data will always automatically purged on your forwarders, in case of unexpected error (such as batch mode non working), this can prevent from blocking the Universal Forwarder (running ouf of file handle)

<br />

The shell script "nmon_cleaner.sh" is now called in default configuration, if Python 2.7.x is found, then the Python version will be used.

If no Python 2.7.x interpreter could locally be found, the Perl version will be automatically launched.

Please see inputs.conf for more information.

<br />
<br />

<li>
<b>resources/Nmon_SplunkApp_Customize.py</b>
</li>

<br />

A Python command line tool to automatically customize the Application to fit your need and criteria: Customizing the index name, App root directory, TA and PA root directory...

<br />
It can be used over future release to allow easy update, even when adapting the App to your needs is required.

</lu>

</p>

	</html>
  </row>
  <row>
    <html>

<div id="FAQ">

<h1>9. FAQ</h1>

</div>

<br />

<div id="FAQ_nmon2csv_debug">

<h2>- Debugging the nmon2csv Python / Perl converter:</h2> 

</div>

In case of trouble, you may be interested in debugging nmon2csv Python / Perl converters operations.

<br />
<br />

This can easily be achieved, either on nmon / TA-nmon / PA-nmon Application:

<br />
<br />

<lu>

<li>
Create a temporary location for csv files, such like the normal directory structure of the App, example: 
</li>

<br />

<pre>
$ mkdir -p /tmp/nmon2csv_debug/etc/apps/nmon
</pre>

<br />
<br />

<li>
Have an nmon file ready to test, if you don't have some to get the current copy in $SPLUNK_HOME/etc/apps/nmon/var/nmon_resposity when the Application is running
</li>

<br />

<li>
Initiate conversion steps:
</li>

<br />
Adapt paths if you want to debug the nmon / TA-nmon / PA-nmon App and the type of Splunk instance (standard, light forwarder, heavy forwarder, peer node...), the following example will reproduce the conversion step for the standard Application:

<br />
<br />

<pre>
$ cd /tmp/nmon2csv_debug
</pre>

<br />
<br />

<pre>
$ export SPLUNK_HOME="/tmp/nmon2csv_debug"
</pre>

<br />
<br />

<b>For Python version</b>

<pre>
$ cat my_file.nmon | /opt/splunk/etc/apps/nmon/bin/nmon2csv.py
</pre>

<b>For Pperl version</b>

<pre>
$ cat my_file.nmon | /opt/splunk/etc/apps/nmon/bin/nmon2csv.pl
</pre>


<br />
<br />

<li>
The converter will output its processing steps and generate various csv files in csv_repository and config_repository
</li>

<br />

<li>
Note that you can achieve the same operation in the proper normal Splunk directory, but if you do so, you need to stop Splunk before as it would immediately index and delete csv files
</li>

</lu>

<br />

<div id="FAQ_customize">

<br />

<h2>- Nmon_SplunkApp_Customize.py: Customize the Application</h2> 

</div>

<b>If for some reason you need to customize the Nmon Splunk Application, A Python command line tool is provided in the resources directory which will help you easily achieving your customizations.</b>

<br />
<br />

The Python tool allows to:

<br />
<br />

<lu>

<li>Customize the Appication Index Name (default: nmon)</li>
<li>Customize the Application Root Directory (default: nmon)</li>
<li>Customize the TA NMON Root Directory (default: TA-nmon)</li>
<li>Customize the PA NMON Root Directory (default: PA-nmon)</li>
<li>Customize the local CSV Repository (default:csv_repository)</li>
<li>Customize the local Config Repository (default:config_repository)</li>

</lu>

<br />

Using this tool over releases, you can easily manage your customizations and update the Application as usual.


<br />
<br />

<pre>

./Nmon_SplunkApp_Customize.py

If for some reason you need to customize the Nmon Splunk Application, please follow these instructions:

- Download the current release of Nmon App in Splunk Base: https://apps.splunk.com/app/1753
- Uncompress the Nmon_SplunkApp_Customize.py.gz
- Place the downloaded tgz Archive and this Python tool in the directory of your choice
- Run the tool: ./customize_indexname.py and check for available options

After the execution, the Application (including TA-nmon and PA-nmon in resources) will have been customized are ready to be used

</pre>

<br />

<pre>

./Nmon_SplunkApp_Customize.py -h

usage: Nmon_SplunkApp_Customize.py [-h] [-f INFILE] [-i INDEX_NAME]
                                   [-r ROOT_DIR] [-a TA_NMON] [-p PA_NMON]
                                   [--csvrepo CSV_REPOSITORY]
                                   [--configrepo CONFIG_REPOSITORY]
                                   [--version]

optional arguments:
  -h, --help            show this help message and exit
  -f INFILE             Name of the Nmon Splunk APP tgz Archive file
  -i INDEX_NAME         Customize the Appication Index Name (default: nmon)
  -r ROOT_DIR           Customize the Application Root Directory (default:
                        nmon)
  -a TA_NMON            Customize the TA NMON Root Directory (default: TA-
                        nmon)
  -p PA_NMON            Customize the PA NMON Root Directory (default: PA-
                        nmon)
  --csvrepo CSV_REPOSITORY
                        Customize the local CSV Repository (default:
                        csv_repository)
  --configrepo CONFIG_REPOSITORY
                        Customize the local Config Repository (default:
                        config_repository)
  --version             show program's version number and exit



</pre>

<br />

<b>Example of utilization:</b>

<br />
<br />

<pre>

./Nmon_SplunkApp_Customize.py -f nmon-performance-monitor-for-unix-and-linux-systems_146.tgz -i my_custom_index -r my_custom_app -a my_custom_ta -p my_custom_pa --csvrepo my_custom_csvrepo --configrepo my_custom_configrepo
Extracting tgz Archive: nmon-performance-monitor-for-unix-and-linux-systems_146.tgz
INFO: Changing the App Root Directory frm default "nmon" to custom "my_custom_app"
Achieving files transformation:
INFO: Customizing any reference to default root directory in files
Achieving files conversion
INFO: Customizing any reference to index name in files
INFO: Customizing indexes.conf
INFO: Customizing csv_repository to my_custom_csvrepo
INFO: Customizing config_repository to my_custom_configrepo
INFO: Removing tgz resources Archives
INFO: Customizing the TA-nmon Root directory from the default TA-nmon to my_custom_ta
INFO: Tar creation done of: my_custom_ta_custom.tar.gz
INFO: Customizing the PA-nmon Root directory from the default PA-nmon to my_custom_pa
INFO: Tar creation done of: my_custom_pa_custom.tar.gz
INFO: Creating the custom nmon_performance_monitor_custom.spl archive in current root directory
INFO: Tar creation done of: nmon_performance_monitor_custom.spl
Operation terminated.

</pre>

<br />
<br />

<div id="FAQ_customize_extraction">

<br />

<h2>- Customize Performance Monitors to be extracted from Nmon data: Focus on Performance Monitors and reduce the Data volume / Licence cost</h2> 

</div>

There may be cases where you don't want or don't need every Performance monitor to be extracted from Nmon Data.

<br />

For example, you want to focus only on particular Monitors such as CPU usage, or you goal is to drastically reduce the global amount of Data volume, and so reduce the licence cost or storage cost.

<br />
<br />

<b>Starting with version 1.4.8</b> you can easily customize the nmon2csv Python converter to choose exact Nmon sections you want to extract.

<br />
<br />

<b>To do so, customize the nmon2csv.py Python converter in the Parameters section:</b>

<br />
<br />

<pre>

#################################################
##      Parameters
#################################################

# Customizations goes here:

# Sections of Performance Monitors with standard dynamic header but no "device" notion that would require the data to be transposed
# You can add or remove any section depending on your needs
static_section = ["LPAR", "CPU_ALL", "FILE", "MEM", "PAGE", "MEMNEW", "MEMUSE", "PROC", "PROCSOL", "VM", "NFSSVRV2",
                  "NFSSVRV3", "NFSSVRV4", "NFSCLIV2", "NFSCLIV3", "NFSCLIV4"]

# This is the TOP section which contains Performance data of top processes
# It has a specific structure and requires specific treatment
top_section = ["TOP"]

# This is the UARG section which contains full command line arguments with some other information such as PID, user, group and so on
# It has a specific structure and requires specific treatment
uarg_section = ["UARG"]

# Sections of Performance Monitors with "device" notion, data needs to be transposed by time to be fully exploitable
# This particular section will check for up to 10 subsection per Performance Monitor
# By default, Nmon create a new subsection (add an increment from 1 to x) per step of 150 devices
# 1500 devices (disks) will be taken in charge in default configuration
dynamic_section1 = ["DISKBUSY", "DISKBSIZE", "DISKREAD", "DISKWRITE", "DISKXFER", "DISKRIO", "DISKWIO"]

# Sections of Performance Monitors with "device" notion, data needs to be transposed by time to be fully exploitable
dynamic_section2 = ["IOADAPT", "NETERROR", "NET", "NETPACKET", "JFSFILE", "JFSINODE"]

</pre>

<br />
<br />

<b>And simply adapt sections to your needs, please note:</b>

<br />
<br />

<lu>

<li>
To remove every monitor of a subsection, empty every monitor of it
</li>

<li>
The customization of the Python nmon2csv converter is not upgrade resilient, remember to reapply your change at every update of the Application
</li>

</lu>

<br />
<br />

<br />

<div id="FAQ_converter">

<h2>- Nmon converter flavour: How to use the alternative Perl converter</h2> 

</div>

<br />

Activating the Perl Nmon converter (nmon2csv.pl) in replacement of the default Python converter is very simple, to be upgrade resilient i recommend to set your configuration in local instead of modifying the default configuration files.

<br />
<br />

<b>Edit the "props.conf" and set:</b>

<br />
<br />

<pre>

##################################
#			nmon2csv stanza			#
##################################

# Source stanza for nmon2csv converter script
# PYTHON converter: The default configuration uses a Python converter "nmon2csv.py"
# PERL alternative: An other converter written in Perl "nmon2csv.pl" can be used in replacement when Python is unavailable, of if you have trouble with it
# Both converters almost share the level of functionalities like inconsistency prevention, statistical information, and operation logging
# You can use either the Python or the Perl converter, change the unarchive_cmd line depending on your needs 

# To use the Python converter, set:
# $SPLUNK_HOME/bin/splunk cmd python $SPLUNK_HOME/etc/apps/nmon/bin/nmon2csv.py

# To use the Perl converter, set:
# $SPLUNK_HOME/etc/apps/nmon/bin/nmon2csv.pl

# This source stanza will be called by the archive processor to convert NMON raw data into csv files
# SPlunk can manage. See inputs.conf for the associated monitor
# The standard nmon App and PA-nmon App will use the embedded splunk interpreter

[source::.../*.nmon]

invalid_cause = archive
unarchive_cmd = $SPLUNK_HOME/etc/apps/nmon/bin/nmon2csv.pl
sourcetype = nmon_processing
NO_BINARY_CHECK = true

</pre>

<br />
<br />

Then restart the Splunk instance, or re-deploy the PA / TA-nmon App to activate the change and you're done.

<br />
<br />

	</html>
  </row>
</form>
